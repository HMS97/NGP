{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huimingsun/anaconda3/envs/NGP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.24s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from video import VideoSeparator\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "from tool import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vs = VideoSeparator( load_data_path = 'data_v1.pkl', path = \"/home/huimingsun/Desktop/RESEARCH_PROJECT/NGP/data/video.mp4\")\n",
    "vs.read_video()\n",
    "frame_list = vs.frames\n",
    "audio_text_result = vs.pack_data['audio_text_result']\n",
    "\n",
    "\n",
    "transformed_audio_result = []\n",
    "for item in audio_text_result:\n",
    "    transformed_audio_result.append([[int(item['start']), int(item['end'])], item['speaker'], item['text']])\n",
    "\n",
    "\n",
    "# processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-6.7b\")\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "#     \"Salesforce/blip2-opt-6.7b\", torch_dtype=torch.float16\n",
    "# ).to(device)\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "\n",
    "resized_video = [cv2.resize(frame, (256, 256)) for frame in vs.frames]\n",
    "resized_video = [Image.fromarray(frame) for frame in resized_video]\n",
    "\n",
    "time_length = 4\n",
    "\n",
    "\n",
    "frame_interval = time_length // 4\n",
    "# set the desired size of the new image\n",
    "new_size = (512, 512)\n",
    "\n",
    "# create a new list to store the modified images\n",
    "modified_video = []\n",
    "\n",
    "# loop through the resized video frames and combine every 4 frames into a new image\n",
    "for i in range(0, len(resized_video), time_length):\n",
    "    # extract 4 frames\n",
    "    # frames = resized_video[i:i+4]\n",
    "    frames = [resized_video[i], resized_video[i+frame_interval], resized_video[i+2*frame_interval], resized_video[i+3*frame_interval]]\n",
    "\n",
    "    # create a new canvas image\n",
    "    canvas = Image.new('RGB', (new_size[0]*2, new_size[1]*2), (255, 255, 255))\n",
    "    # paste the frames onto the canvas\n",
    "    for j in range(4):\n",
    "        x = (j % 2) * new_size[0]\n",
    "        y = (j // 2) * new_size[1]\n",
    "        # resize the frame to fit the canvas\n",
    "        resized_frame = frames[j].resize(new_size)\n",
    "        canvas.paste(resized_frame, (x, y))\n",
    "    # add the modified image to the list\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "    draw.line((new_size[0], 0, new_size[0], new_size[1]*2), fill=(255, 255, 255), width=4)\n",
    "    draw.line((0, new_size[1], new_size[0]*2, new_size[1]), fill=(255, 255, 255), width=4)\n",
    "    canvas = cv2.resize(np.array(canvas), (512, 512))\n",
    "    modified_video.append(canvas)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multi_frame_Video_caption(time, modified_video,time_length):\n",
    "\n",
    "    # summary_text = f'The vision summary for frame {time} to {time+time_length} is: '\n",
    "    summary_text = f'The vision summary is: '\n",
    "\n",
    "    time = time // time_length\n",
    "\n",
    "    questions = [None,\"Describe what happend in the image\", \"Describe the first scene.\", \"Describe the second scene.\", \"Describe the third scene.\", \"Describe the forth scene.\"]\n",
    "    scene_list = ['overall scenes', 'overall scenes', 'The first scene', 'The second scene ', 'The third scene ', 'The fourth scene ']\n",
    "    for index,question in enumerate(questions):\n",
    "\n",
    "        prompt = f\"Question:{question} Answer:\" if question else None\n",
    "        # prompt = None\n",
    "\n",
    "        inputs = processor(images=modified_video[time], text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "        generated_ids = model.generate(**inputs)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        generated_text = generated_text.replace('scene', \" \" + str(scene_list[index])+' ')\n",
    "\n",
    "        # for index_i, i in enumerate(['The first scene is', 'The second scene is', 'The third scene is', 'The fourth scene is']):\n",
    "        #     index_i += 1\n",
    "        summary_text += generated_text \n",
    "    return summary_text\n",
    "\n",
    "\n",
    "def find_near_speech_text(speech_data, time_range, speaker_time = None):\n",
    "    near_text_data = ' '\n",
    "\n",
    "    for data in speech_data:\n",
    "        start_time, end_time = data[0]\n",
    "        speaker = data[1]\n",
    "        text = data[2]\n",
    "        if ( time_range[0] <= start_time <= time_range[1]  ):\n",
    "            if speaker is None: speaker = 'UNKNOW'\n",
    "            if speaker_time != None: \n",
    "                string = f' At frame {start_time} to {end_time}, {speaker} says: {text} '\n",
    "            else:\n",
    "                string = f' {speaker} says: {text} '\n",
    "            near_text_data += string\n",
    "\n",
    "    return str(near_text_data)\n",
    "\n",
    "\n",
    "\n",
    "def combine_speech_video(time,modified_video,transformed_audio_result, time_length = 4):\n",
    "    visual_text =multi_frame_Video_caption(time = time, modified_video = modified_video, time_length = time_length)\n",
    "    speech_text = find_near_speech_text(transformed_audio_result, time_range= [time, time+time_length], speaker_time = None)\n",
    "    \n",
    "    all_text =   f' Frame {time} to {time+time_length}:' + visual_text + speech_text + f\" Frame {time} to {time+time_length} text end. \"\n",
    "    visual_text =   f' Frame {time} to {time+time_length}: ' + visual_text + f\" Frame {time} to {time+time_length} text end. \"\n",
    "    speech_text =   f' Frame {time} to {time+time_length}:  speech_text: '  + speech_text + f\" Frame {time} to {time+time_length} text end. \"\n",
    "\n",
    "    \n",
    "    \n",
    "    return all_text, visual_text, speech_text\n",
    "\n",
    "\n",
    "all_speech_text = find_near_speech_text(transformed_audio_result, time_range= [0, len(resized_video)], speaker_time = True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [05:49<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_summary_text = []\n",
    "all_visual_text = []\n",
    "# with get_openai_callback() as cb:\n",
    "for index,time in enumerate(tqdm(range(0, len(resized_video), time_length))):\n",
    "    all_text, visual_text, speech_text = combine_speech_video(time = time, modified_video = modified_video, transformed_audio_result = transformed_audio_result, time_length = time_length)\n",
    "    all_summary_text.append(all_text)\n",
    "    all_visual_text.append(visual_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "#list to string \n",
    "continus_summary_text = ' '.join(all_summary_text)\n",
    "continus_visual_text = ' '.join(all_visual_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    pattern = r\"(Frame \\d+ to \\d+:)\"\n",
    "    segments = re.split(pattern, text)\n",
    "    cleaned_segments = []\n",
    "\n",
    "    for i in range(1, len(segments), 2):\n",
    "        cleaned_segments.append(segments[i] + segments[i+1])\n",
    "\n",
    "    return cleaned_segments\n",
    "\n",
    "ll = split_text(continus_summary_text)\n",
    "\n",
    "# continus_summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_speech_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "264\n",
      "264\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "numbers = num_tokens_from_string(all_speech_text,\"cl100k_base\" )\n",
    "split_speech_file('data/speech_data', all_speech_text,( numbers//800 + 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numbers = num_tokens_from_string(continus_visual_text,\"cl100k_base\" )\n",
    "split_video_file('data/vision_data', continus_visual_text,( numbers//2300 + 1))\n",
    "\n",
    "\n",
    "\n",
    "numbers = num_tokens_from_string(continus_summary_text,\"cl100k_base\" )\n",
    "            \n",
    "split_video_file('data/summary_data', continus_summary_text,( numbers//2300 + 1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers//1100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('NGP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78c4147ab13163a3c00273e60ee5355a078dd96e1cc9666865d7ea134c5f88b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
